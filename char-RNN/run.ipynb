{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "# 1. .txt 파일 읽어들인다.\n",
    "# 2. 전처리 파일이 있으면 전처리된 파일 로드, 없으면 전처리후 파일 저장\n",
    "#\n",
    "# 전처리 파일\n",
    "# vocab: vocabulary dic {'char': idx}\n",
    "# tensor: 읽어들인 파일의 문자들을 vocab 기준으로 숫자로 변환한 배열. (None, ) size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 파일 로드중...\n",
      "배치 생성 완료\n"
     ]
    }
   ],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, src_dir, batch_size, seq_length):\n",
    "        self.vocab = None\n",
    "        self.vocab_size = None\n",
    "        self.tensor = None\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        shake = 'tinyshakespeare/input.txt'\n",
    "        sample = 'sample.txt'\n",
    "        self.src_path = os.path.join(src_dir, sample)\n",
    "        self.vocab_path = os.path.join(src_dir, 'vocab.pkl')\n",
    "        self.tensor_path = os.path.join(src_dir, 'tensor.npy')\n",
    "        \n",
    "        if os.path.exists(self.vocab_path) and os.path.exists(self.tensor_path):\n",
    "            print('전처리된 파일 로드중...')\n",
    "            self.load_preprocessed()\n",
    "        else:\n",
    "            print('데이터 전처리중...')\n",
    "            self.preprocess()\n",
    "            \n",
    "        self.create_batch()\n",
    "        self.reset_batch_pointer()\n",
    "        print('배치 생성 완료')\n",
    "        \n",
    "    def preprocess(self):\n",
    "        # 파일 읽기\n",
    "        with open(self.src_path, 'r') as fp:\n",
    "            data = fp.read()\n",
    "        print(data)\n",
    "        self.chars = sorted(set(data))                           # ['a','b','c'...]\n",
    "        self.vocab_size = len(chars)                             # vocab size\n",
    "        self.vocab = {j:i for i, j in enumerate(self.chars)}     # vocab dic {'char': idx}\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))  # tensor ('char'->idx)\n",
    "        # 파일 저장\n",
    "        with open(self.vocab_path, 'wb') as fp:\n",
    "            pickle.dump(self.vocab, fp)\n",
    "        np.save(self.tensor_path, self.tensor)\n",
    "            \n",
    "    def load_preprocessed(self):\n",
    "        with open(self.vocab_path, 'rb') as fp:\n",
    "            self.vocab = pickle.load(fp)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.chars = sorted(self.vocab.keys())\n",
    "        self.tensor = np.load(self.tensor_path)\n",
    "        \n",
    "    def create_batch(self):\n",
    "        self.total_batch = self.tensor.size//(self.batch_size*self.seq_length)\n",
    "        if self.total_batch == 0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "        self.tensor = self.tensor[:self.batch_size*self.seq_length*self.total_batch]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        # ydata 를 xdata 를 한칸 왼쪽으로 쉬프트한 형태로 구성.\n",
    "        # b  c  d  a (ydata)\n",
    "        # ==rnn cell==\n",
    "        # a  b  c  d (xdata)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        \n",
    "        self.x_batches =  np.split(xdata.reshape(self.batch_size, -1), self.total_batch, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.total_batch, 1)\n",
    "        \n",
    "    # 배치 불러오고 포인터를 1만큼 증가.\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x,y\n",
    "    \n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0\n",
    "    \n",
    "    def convert_idx2char(self, num):\n",
    "        char = self.chars[num]\n",
    "        return char\n",
    "\n",
    "    \n",
    "    \n",
    "data_dir = 'data'\n",
    "batch_size = 16\n",
    "seq_length = 16\n",
    "hidden_size = 64\n",
    "learning_rate = 0.05\n",
    "num_epoch = 2\n",
    "num_hidden_layers = 2\n",
    "grad_clip = 5\n",
    "\n",
    "data_loader = TextLoader(data_dir, batch_size, seq_length)\n",
    "\n",
    "chars = data_loader.chars\n",
    "vocab = data_loader.vocab\n",
    "vocab_size = data_loader.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph\n",
    "\n",
    "# reset graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 인풋/타겟 데이터, 배치 사이즈를 입력받기 위한 플레이스홀더\n",
    "input_data = tf.placeholder(tf.int32, shape=[None, None])  # input: [batch_size, seq_length]\n",
    "target_data = tf.placeholder(tf.int32, shape=[None, None]) # target: [batch_size, seq_length]\n",
    "state_batch_size = tf.placeholder(tf.int32, shape=[])\n",
    "\n",
    "# RNN 마지막 히든레이어 출력을 소프트맥스 출력값으로 변환해주기 위한 변수\n",
    "softmax_w = tf.Variable(tf.random_normal(shape=[hidden_size, vocab_size]), dtype=tf.float32)\n",
    "softmax_b = tf.Variable(tf.random_normal(shape=[vocab_size]), dtype=tf.float32)\n",
    "\n",
    "# 히든레이어 수 만큼 LSTM cell(히든레이어) 선언\n",
    "cells = []\n",
    "for _ in range(num_hidden_layers):\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "    cells.append(cell)\n",
    "    \n",
    "# cell을 종합해서 RNN을 정의\n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "# 인풋데이터를 변환하기 위한 임베딩 매트릭스 선언\n",
    "# vocab_size -> hidden_size\n",
    "embedding = tf.Variable(tf.random_normal(shape=[vocab_size, hidden_size]), dtype=tf.float32)\n",
    "inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "\n",
    "# 초기 state 값을 0으로 초기화\n",
    "initial_state = cell.zero_state(state_batch_size, tf.float32)\n",
    "\n",
    "# 학습을 위한 tf.nn.dynamic_rnn을 선언\n",
    "# outputs: [batch_size, seq_length, hidden_size]\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# output을 [batch_size * seq_length, hidden_size] shape으로 바꿈\n",
    "output = tf.reshape(outputs, [-1, hidden_size])\n",
    "\n",
    "# 최종 출력값을 설정\n",
    "# logits: [batch_size * seq_length, vocab_size]\n",
    "# softmax 를 적용하기 위해 vocab_size 로 shape 을 바꾼다.\n",
    "# output.shape: (?,64)\n",
    "# logits.shape: (?,36)\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "probs = tf.nn.softmax(logits)\n",
    "\n",
    "# 크로스 엔트로피 손실함수 정의\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=target_data))\n",
    "\n",
    "# 옵티마이저 선언하고 옵티마이저에 Gradient Clipping을 적용\n",
    "# grad_clip 보다 큰 Gradient를 5로 Clipping 한다\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "# 세션을 열고 학습 진행\n",
    "with tf.Session() as sess:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
