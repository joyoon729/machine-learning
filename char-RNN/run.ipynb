{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextLoader\n",
    "# 1. .txt 파일 읽어들인다.\n",
    "# 2. 전처리 파일이 있으면 전처리된 파일 로드, 없으면 전처리후 파일 저장\n",
    "#\n",
    "# 전처리 파일\n",
    "# vocab: vocabulary dic {'char': idx}\n",
    "# tensor: 읽어들인 파일의 문자들을 vocab 기준으로 숫자로 변환한 배열. (None, ) size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 파일 로드중...\n",
      "[1] Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on artificial neural networks. Learning can be supervis\n",
      "배치 생성 완료\n"
     ]
    }
   ],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, src_dir, batch_size, seq_length):\n",
    "        self.vocab = None\n",
    "        self.vocab_size = None\n",
    "        self.tensor = None\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        shake = 'tinyshakespeare'\n",
    "        sample = ''\n",
    "        src_dir = os.path.join(src_dir, sample)\n",
    "        self.src_path = os.path.join(src_dir, 'input.txt')\n",
    "        self.vocab_path = os.path.join(src_dir, 'vocab.pkl')\n",
    "        self.tensor_path = os.path.join(src_dir, 'tensor.npy')\n",
    "        \n",
    "        if os.path.exists(self.vocab_path) and os.path.exists(self.tensor_path):\n",
    "            print('전처리된 파일 로드중...')\n",
    "            self.load_preprocessed()\n",
    "        else:\n",
    "            print('데이터 전처리중...')\n",
    "            self.preprocess()\n",
    "            \n",
    "        self.create_batch()\n",
    "        self.reset_batch_pointer()\n",
    "        \n",
    "        for i, line in enumerate(self.tensor):\n",
    "            print(self.chars[line], end='')\n",
    "            if i==200: break\n",
    "        print()\n",
    "        print('배치 생성 완료')\n",
    "        \n",
    "    def preprocess(self):\n",
    "        # 파일 읽기\n",
    "        with open(self.src_path, 'r') as fp:\n",
    "            data = fp.read()\n",
    "        self.chars = sorted(set(data))                           # ['a','b','c'...]\n",
    "        self.vocab_size = len(chars)                             # vocab size\n",
    "        self.vocab = {j:i for i, j in enumerate(self.chars)}     # vocab dic {'char': idx}\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))  # tensor ('char'->idx)\n",
    "        # 파일 저장\n",
    "        with open(self.vocab_path, 'wb') as fp:\n",
    "            pickle.dump(self.vocab, fp)\n",
    "        np.save(self.tensor_path, self.tensor)\n",
    "            \n",
    "    def load_preprocessed(self):\n",
    "        with open(self.vocab_path, 'rb') as fp:\n",
    "            self.vocab = pickle.load(fp)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.chars = sorted(self.vocab.keys())\n",
    "        self.tensor = np.load(self.tensor_path)\n",
    "        \n",
    "    def create_batch(self):\n",
    "        self.total_batch = self.tensor.size//(self.batch_size*self.seq_length)\n",
    "        if self.total_batch == 0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "        self.tensor = self.tensor[:self.batch_size*self.seq_length*self.total_batch]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        # ydata 를 xdata 를 한칸 왼쪽으로 쉬프트한 형태로 구성.\n",
    "        # b  c  d  a (ydata)\n",
    "        # ==rnn cell==\n",
    "        # a  b  c  d (xdata)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        \n",
    "        self.x_batches =  np.split(xdata.reshape(self.batch_size, -1), self.total_batch, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.total_batch, 1)\n",
    "        \n",
    "    # 배치 불러오고 포인터를 1만큼 증가.\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x,y\n",
    "    \n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0\n",
    "    \n",
    "    def convert_idx2char(self, num):\n",
    "        char = self.chars[num]\n",
    "        return char\n",
    "\n",
    "    \n",
    "    \n",
    "data_dir = 'data'\n",
    "batch_size = 32\n",
    "seq_length = 20\n",
    "data_loader = TextLoader(data_dir, batch_size, seq_length)\n",
    "\n",
    "chars = data_loader.chars\n",
    "# print(chars)\n",
    "vocab = data_loader.vocab\n",
    "vocab_size = data_loader.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 epoch  |  [ 1/1] batch\t|\tloss: 4.8277054\n",
      "  2 epoch  |  [ 1/1] batch\t|\tloss: 8.5346355\n",
      "  3 epoch  |  [ 1/1] batch\t|\tloss: 12.8293400\n",
      "  4 epoch  |  [ 1/1] batch\t|\tloss: 9.4195995\n",
      "  5 epoch  |  [ 1/1] batch\t|\tloss: 8.4244556\n",
      "  6 epoch  |  [ 1/1] batch\t|\tloss: 7.3591895\n",
      "  7 epoch  |  [ 1/1] batch\t|\tloss: 6.1464701\n",
      "  8 epoch  |  [ 1/1] batch\t|\tloss: 4.3289132\n",
      "  9 epoch  |  [ 1/1] batch\t|\tloss: 3.5216179\n",
      " 10 epoch  |  [ 1/1] batch\t|\tloss: 2.9781959\n",
      " 11 epoch  |  [ 1/1] batch\t|\tloss: 2.9047246\n",
      " 12 epoch  |  [ 1/1] batch\t|\tloss: 2.6793096\n",
      " 13 epoch  |  [ 1/1] batch\t|\tloss: 2.4923337\n",
      " 14 epoch  |  [ 1/1] batch\t|\tloss: 2.4245381\n",
      " 15 epoch  |  [ 1/1] batch\t|\tloss: 2.3181529\n",
      " 16 epoch  |  [ 1/1] batch\t|\tloss: 2.1905053\n",
      " 17 epoch  |  [ 1/1] batch\t|\tloss: 2.0425019\n",
      " 18 epoch  |  [ 1/1] batch\t|\tloss: 1.9409739\n",
      " 19 epoch  |  [ 1/1] batch\t|\tloss: 1.8402655\n",
      " 20 epoch  |  [ 1/1] batch\t|\tloss: 1.7380224\n",
      " 21 epoch  |  [ 1/1] batch\t|\tloss: 1.6446555\n",
      " 22 epoch  |  [ 1/1] batch\t|\tloss: 1.5562851\n",
      " 23 epoch  |  [ 1/1] batch\t|\tloss: 1.4717138\n",
      " 24 epoch  |  [ 1/1] batch\t|\tloss: 1.3913581\n",
      " 25 epoch  |  [ 1/1] batch\t|\tloss: 1.3167305\n",
      " 26 epoch  |  [ 1/1] batch\t|\tloss: 1.2468575\n",
      " 27 epoch  |  [ 1/1] batch\t|\tloss: 1.1770203\n",
      " 28 epoch  |  [ 1/1] batch\t|\tloss: 1.1061997\n",
      " 29 epoch  |  [ 1/1] batch\t|\tloss: 1.0388294\n",
      " 30 epoch  |  [ 1/1] batch\t|\tloss: 0.9737711\n",
      " 31 epoch  |  [ 1/1] batch\t|\tloss: 0.9109060\n",
      " 32 epoch  |  [ 1/1] batch\t|\tloss: 0.8533920\n",
      " 33 epoch  |  [ 1/1] batch\t|\tloss: 0.7967300\n",
      " 34 epoch  |  [ 1/1] batch\t|\tloss: 0.7364047\n",
      " 35 epoch  |  [ 1/1] batch\t|\tloss: 0.6804873\n",
      " 36 epoch  |  [ 1/1] batch\t|\tloss: 0.6293513\n",
      " 37 epoch  |  [ 1/1] batch\t|\tloss: 0.5817168\n",
      " 38 epoch  |  [ 1/1] batch\t|\tloss: 0.5368029\n",
      " 39 epoch  |  [ 1/1] batch\t|\tloss: 0.4941982\n",
      " 40 epoch  |  [ 1/1] batch\t|\tloss: 0.4549285\n",
      " 41 epoch  |  [ 1/1] batch\t|\tloss: 0.4179459\n",
      " 42 epoch  |  [ 1/1] batch\t|\tloss: 0.3830229\n",
      " 43 epoch  |  [ 1/1] batch\t|\tloss: 0.3512322\n",
      " 44 epoch  |  [ 1/1] batch\t|\tloss: 0.3213950\n",
      " 45 epoch  |  [ 1/1] batch\t|\tloss: 0.2922304\n",
      " 46 epoch  |  [ 1/1] batch\t|\tloss: 0.2657450\n",
      " 47 epoch  |  [ 1/1] batch\t|\tloss: 0.2415101\n",
      " 48 epoch  |  [ 1/1] batch\t|\tloss: 0.2191449\n",
      " 49 epoch  |  [ 1/1] batch\t|\tloss: 0.1991709\n",
      " 50 epoch  |  [ 1/1] batch\t|\tloss: 0.1808790\n",
      " 51 epoch  |  [ 1/1] batch\t|\tloss: 0.1648617\n",
      " 52 epoch  |  [ 1/1] batch\t|\tloss: 0.1508733\n",
      " 53 epoch  |  [ 1/1] batch\t|\tloss: 0.1383766\n",
      " 54 epoch  |  [ 1/1] batch\t|\tloss: 0.1272222\n",
      " 55 epoch  |  [ 1/1] batch\t|\tloss: 0.1175193\n",
      " 56 epoch  |  [ 1/1] batch\t|\tloss: 0.1093301\n",
      " 57 epoch  |  [ 1/1] batch\t|\tloss: 0.1019709\n",
      " 58 epoch  |  [ 1/1] batch\t|\tloss: 0.0954826\n",
      " 59 epoch  |  [ 1/1] batch\t|\tloss: 0.0900009\n",
      " 60 epoch  |  [ 1/1] batch\t|\tloss: 0.0852075\n",
      " 61 epoch  |  [ 1/1] batch\t|\tloss: 0.0810363\n",
      " 62 epoch  |  [ 1/1] batch\t|\tloss: 0.0774924\n",
      " 63 epoch  |  [ 1/1] batch\t|\tloss: 0.0744795\n",
      " 64 epoch  |  [ 1/1] batch\t|\tloss: 0.0719097\n",
      " 65 epoch  |  [ 1/1] batch\t|\tloss: 0.0696359\n",
      " 66 epoch  |  [ 1/1] batch\t|\tloss: 0.0675388\n",
      " 67 epoch  |  [ 1/1] batch\t|\tloss: 0.0656888\n",
      " 68 epoch  |  [ 1/1] batch\t|\tloss: 0.0641235\n",
      " 69 epoch  |  [ 1/1] batch\t|\tloss: 0.0627776\n",
      " 70 epoch  |  [ 1/1] batch\t|\tloss: 0.0615810\n",
      " 71 epoch  |  [ 1/1] batch\t|\tloss: 0.0605103\n",
      " 72 epoch  |  [ 1/1] batch\t|\tloss: 0.0595812\n",
      " 73 epoch  |  [ 1/1] batch\t|\tloss: 0.0587519\n",
      " 74 epoch  |  [ 1/1] batch\t|\tloss: 0.0580197\n",
      " 75 epoch  |  [ 1/1] batch\t|\tloss: 0.0573543\n",
      " 76 epoch  |  [ 1/1] batch\t|\tloss: 0.0567526\n",
      " 77 epoch  |  [ 1/1] batch\t|\tloss: 0.0562056\n",
      " 78 epoch  |  [ 1/1] batch\t|\tloss: 0.0557105\n",
      " 79 epoch  |  [ 1/1] batch\t|\tloss: 0.0552639\n",
      " 80 epoch  |  [ 1/1] batch\t|\tloss: 0.0548645\n",
      " 81 epoch  |  [ 1/1] batch\t|\tloss: 0.0545048\n",
      " 82 epoch  |  [ 1/1] batch\t|\tloss: 0.0541679\n",
      " 83 epoch  |  [ 1/1] batch\t|\tloss: 0.0538558\n",
      " 84 epoch  |  [ 1/1] batch\t|\tloss: 0.0535621\n",
      " 85 epoch  |  [ 1/1] batch\t|\tloss: 0.0532887\n",
      " 86 epoch  |  [ 1/1] batch\t|\tloss: 0.0530322\n",
      " 87 epoch  |  [ 1/1] batch\t|\tloss: 0.0527950\n",
      " 88 epoch  |  [ 1/1] batch\t|\tloss: 0.0525700\n",
      " 89 epoch  |  [ 1/1] batch\t|\tloss: 0.0523605\n",
      " 90 epoch  |  [ 1/1] batch\t|\tloss: 0.0521613\n",
      " 91 epoch  |  [ 1/1] batch\t|\tloss: 0.0519755\n",
      " 92 epoch  |  [ 1/1] batch\t|\tloss: 0.0518012\n",
      " 93 epoch  |  [ 1/1] batch\t|\tloss: 0.0516388\n",
      " 94 epoch  |  [ 1/1] batch\t|\tloss: 0.0514854\n",
      " 95 epoch  |  [ 1/1] batch\t|\tloss: 0.0513419\n",
      " 96 epoch  |  [ 1/1] batch\t|\tloss: 0.0512057\n",
      " 97 epoch  |  [ 1/1] batch\t|\tloss: 0.0510770\n",
      " 98 epoch  |  [ 1/1] batch\t|\tloss: 0.0509548\n",
      " 99 epoch  |  [ 1/1] batch\t|\tloss: 0.0508379\n",
      "100 epoch  |  [ 1/1] batch\t|\tloss: 0.0507268\n",
      "학습 끝\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "learning_rate = 0.02\n",
    "num_epochs = 100\n",
    "num_hidden_layers = 2\n",
    "grad_clip = 5\n",
    "\n",
    "## graph\n",
    "\n",
    "# reset graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 인풋/타겟 데이터, 배치 사이즈를 입력받기 위한 플레이스홀더\n",
    "input_data = tf.placeholder(tf.int32, shape=[None, None])  # input: [batch_size, seq_length]\n",
    "target_data = tf.placeholder(tf.int32, shape=[None, None]) # target: [batch_size, seq_length]\n",
    "state_batch_size = tf.placeholder(tf.int32, shape=[])\n",
    "\n",
    "# RNN 마지막 히든레이어 출력을 소프트맥스 출력값으로 변환해주기 위한 변수\n",
    "softmax_w = tf.Variable(tf.random_normal(shape=[hidden_size, vocab_size]), dtype=tf.float32)\n",
    "softmax_b = tf.Variable(tf.random_normal(shape=[vocab_size]), dtype=tf.float32)\n",
    "\n",
    "# 히든레이어 수 만큼 LSTM cell(히든레이어) 선언\n",
    "cells = []\n",
    "for _ in range(num_hidden_layers):\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "    cells.append(cell)\n",
    "    \n",
    "# cell을 종합해서 RNN을 정의\n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "# 인풋데이터를 변환하기 위한 임베딩 매트릭스 선언\n",
    "# vocab_size -> hidden_size\n",
    "embedding = tf.Variable(tf.random_normal(shape=[vocab_size, hidden_size]), dtype=tf.float32)\n",
    "inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "\n",
    "# 초기 state 값을 0으로 초기화\n",
    "initial_state = cell.zero_state(state_batch_size, tf.float32)\n",
    "\n",
    "# 학습을 위한 tf.nn.dynamic_rnn을 선언\n",
    "# outputs: [batch_size, seq_length, hidden_size]\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# output을 [batch_size * seq_length, hidden_size] shape으로 바꿈\n",
    "output = tf.reshape(outputs, [-1, hidden_size])\n",
    "\n",
    "# 최종 출력값을 설정\n",
    "# logits: [batch_size * seq_length, vocab_size]\n",
    "# softmax 를 적용하기 위해 vocab_size 로 shape 을 바꾼다.\n",
    "# output.shape: (?,64)\n",
    "# logits.shape: (?,36)\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "probs = tf.nn.softmax(logits)\n",
    "\n",
    "# 크로스 엔트로피 손실함수 정의\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=target_data))\n",
    "\n",
    "# 옵티마이저 선언하고 옵티마이저에 Gradient Clipping을 적용\n",
    "# grad_clip 보다 큰 Gradient를 5로 Clipping 한다\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "## 세션을 열고 학습 진행\n",
    "# with tf.Session() as sess:\n",
    "sess = tf.Session()\n",
    "# 변수에 초기값 할당\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    data_loader.reset_batch_pointer() # TextLoader 의 배치포인터 리셋\n",
    "    # 초기 상태값 지정\n",
    "    state = sess.run(initial_state, feed_dict={state_batch_size: batch_size})\n",
    "\n",
    "    for b in range(data_loader.total_batch):\n",
    "        # x,y 데이터 불러오기\n",
    "        x, y = data_loader.next_batch()\n",
    "        # y에 one-hot 인코딩 적용\n",
    "        y = tf.one_hot(y, vocab_size)        # y: [batch_size, seq_length, vocab_size]\n",
    "        y = tf.reshape(y, [-1, vocab_size])  # y: [batch_size * seq_length, vocab_size]\n",
    "        y = y.eval(session=sess)\n",
    "\n",
    "        # feed_dict 에 사용할 값과 LSTM 초기 cell state(feed_dict[c])값과 hidden layer 출력값(feed_dict[h])를 지정\n",
    "        feed_dict = {input_data: x, target_data: y, state_batch_size: batch_size}\n",
    "        for i, (c, h) in enumerate(initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "\n",
    "        # 1스텝 학습을 진행\n",
    "        _, loss_print, state = sess.run([train_step, loss, final_state], feed_dict=feed_dict)\n",
    "\n",
    "        print(f'{e+1:3d} epoch  |  [{b+1:2d}/{data_loader.total_batch}] batch\\t|\\tloss: {loss_print:.7f}')\n",
    "\n",
    "print('학습 끝')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플링 시작\n",
      "to fields including inspech red beech recognition, natroal neural network and vion, stroal network and convoltupervised.\n",
      "\n",
      "[22222]2eeeep reecurrnted neural networks, family of a broage procesing, recognition, natroan b o besupervised.\n",
      "\n",
      "[2222222]eeeep learning (al networks, nateralnading or unsupervised.\n",
      "\n",
      "[2222]2eeep Deep learning (al learning filtering, machine tdio on artificial network and or unsupervised.\n",
      "\n",
      "[2222]2eeep \n",
      "eep learning archine teural networks, design, machine to filtering, machine tured semi-supervised supervised such as deep stroal neural network and or ader boad game[ter vision, spervised.\n",
      "\n",
      "[2222ee[ eep neural networks, analy of audio recognition, natering or unsupervised.\n",
      "\n",
      "[2222]2ee]eeh perec recurrent neural networks, fan be supervised.\n",
      "\n",
      "[22222]eeeep learning or unsupervised.\n",
      "\n",
      "[22222]2eeeep recurrent neural neural networks, as deep stroal network and convoluturesudo social network and board game[t[. .een applied neural network and board game[[g euresuperviision, sper"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "saver = tf.train.Saver()\n",
    "save_file = 'model.ckpt'\n",
    "saver.save(sess, save_file)\n",
    "\n",
    "# 샘플링 시작\n",
    "print('샘플링 시작')\n",
    "num_sampling = 1000   # 생성할 글자 수 지정\n",
    "prime = u' '          # 시작 글자를 ' '(공백)으로 지정\n",
    "sampling_type = 1     # 샘플링 타입 설정\n",
    "state = sess.run(cell.zero_state(1, tf.float32)) # RNN 최초 state값을 0으로 초기화\n",
    "\n",
    "# 랜덤 샘플링을 위한 weight_pick 함수 정의\n",
    "def weighted_pick(weights):\n",
    "    t = np.cumsum(weights)\n",
    "    s = np.sum(weights)\n",
    "    return int(np.searchsorted(t, np.random.rand(1)*s))\n",
    "\n",
    "ret = prime\n",
    "char = prime[-1]\n",
    "for n in range(num_sampling):\n",
    "    x = np.zeros((1,1))\n",
    "    x[0,0] = vocab[char]\n",
    "    \n",
    "    # RNN 1스텝 실행하고 softmax 행렬을 리턴받는다\n",
    "    feed_dict = {input_data:x, state_batch_size:1, initial_state: state}\n",
    "    [probs_result, state] = sess.run([probs, final_state], feed_dict=feed_dict)\n",
    "\n",
    "    # 불필요한 차원 제거\n",
    "    # probs_result: (1,vocab_size) -> p: (vocab_size)\n",
    "    p = np.squeeze(probs_result)\n",
    "    \n",
    "    # 샘플링 타입에 따라 3가지 종류로 샘플링\n",
    "    # sampling_type: 0 => 다음 글자 예측시 항상 argmax 사용\n",
    "    # sampling_type: 1(default) => 다음 글자 예측시 항상 random sampling 사용\n",
    "    # sampling_type: 2 => 다음 글자 예측시 이전 글자가 ' '(공백) 이면 random sampling, 그렇지 않으면 argmax 사용\n",
    "    if sampling_type == 0:\n",
    "        sample = np.argmax(p)\n",
    "    elif sampling_type == 2:\n",
    "        if char == ' ':\n",
    "            sample = weighted_pick(p)\n",
    "        else:\n",
    "            sample = np.argmax(p)\n",
    "    else:\n",
    "        sample = weighted_pick(p)\n",
    "    \n",
    "    pred = chars[sample]\n",
    "#     ret += pred\n",
    "    char = pred # 예측한 글자를 다음 RNN 의 인풋으로\n",
    "    print(pred, end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
