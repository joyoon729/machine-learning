{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextLoader\n",
    "# 1. .txt 파일 읽어들인다.\n",
    "# 2. 전처리 파일이 있으면 전처리된 파일 로드, 없으면 전처리후 파일 저장\n",
    "#\n",
    "# 전처리 파일\n",
    "# vocab: vocabulary dic {'char': idx}\n",
    "# tensor: 읽어들인 파일의 문자들을 vocab 기준으로 숫자로 변환한 배열. (None, ) size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAIN\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\MAIN\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\MAIN\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\MAIN\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\MAIN\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\MAIN\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\MAIN\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\MAIN\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\MAIN\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\MAIN\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\MAIN\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\MAIN\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 파일 로드중...\n",
      "배치 생성 완료\n"
     ]
    }
   ],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, src_dir, batch_size, seq_length):\n",
    "        self.vocab = None\n",
    "        self.vocab_size = None\n",
    "        self.tensor = None\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        shake = 'tinyshakespeare/input.txt'\n",
    "        sample = 'sample.txt'\n",
    "        self.src_path = os.path.join(src_dir, sample)\n",
    "        self.vocab_path = os.path.join(src_dir, 'vocab.pkl')\n",
    "        self.tensor_path = os.path.join(src_dir, 'tensor.npy')\n",
    "        \n",
    "        if os.path.exists(self.vocab_path) and os.path.exists(self.tensor_path):\n",
    "            print('전처리된 파일 로드중...')\n",
    "            self.load_preprocessed()\n",
    "        else:\n",
    "            print('데이터 전처리중...')\n",
    "            self.preprocess()\n",
    "            \n",
    "        self.create_batch()\n",
    "        self.reset_batch_pointer()\n",
    "        print('배치 생성 완료')\n",
    "        \n",
    "    def preprocess(self):\n",
    "        # 파일 읽기\n",
    "        with open(self.src_path, 'r') as fp:\n",
    "            data = fp.read()\n",
    "        self.chars = sorted(set(data))                           # ['a','b','c'...]\n",
    "        self.vocab_size = len(chars)                             # vocab size\n",
    "        self.vocab = {j:i for i, j in enumerate(self.chars)}     # vocab dic {'char': idx}\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))  # tensor ('char'->idx)\n",
    "        # 파일 저장\n",
    "        with open(self.vocab_path, 'wb') as fp:\n",
    "            pickle.dump(self.vocab, fp)\n",
    "        np.save(self.tensor_path, self.tensor)\n",
    "            \n",
    "    def load_preprocessed(self):\n",
    "        with open(self.vocab_path, 'rb') as fp:\n",
    "            self.vocab = pickle.load(fp)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.chars = sorted(self.vocab.keys())\n",
    "        self.tensor = np.load(self.tensor_path)\n",
    "        \n",
    "    def create_batch(self):\n",
    "        self.total_batch = self.tensor.size//(self.batch_size*self.seq_length)\n",
    "        if self.total_batch == 0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "        self.tensor = self.tensor[:self.batch_size*self.seq_length*self.total_batch]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        # ydata 를 xdata 를 한칸 왼쪽으로 쉬프트한 형태로 구성.\n",
    "        # b  c  d  a (ydata)\n",
    "        # ==rnn cell==\n",
    "        # a  b  c  d (xdata)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        \n",
    "        self.x_batches =  np.split(xdata.reshape(self.batch_size, -1), self.total_batch, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.total_batch, 1)\n",
    "        \n",
    "    # 배치 불러오고 포인터를 1만큼 증가.\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x,y\n",
    "    \n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0\n",
    "    \n",
    "    def convert_idx2char(self, num):\n",
    "        char = self.chars[num]\n",
    "        return char\n",
    "\n",
    "    \n",
    "    \n",
    "data_dir = 'data'\n",
    "batch_size = 16\n",
    "seq_length = 16\n",
    "data_loader = TextLoader(data_dir, batch_size, seq_length)\n",
    "\n",
    "chars = data_loader.chars\n",
    "vocab = data_loader.vocab\n",
    "vocab_size = data_loader.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 epoch  |  [ 1/2] batch\t|\tloss: 3.8348196\n",
      " 1 epoch  |  [ 2/2] batch\t|\tloss: 9.3221636\n",
      " 2 epoch  |  [ 1/2] batch\t|\tloss: 10.5652666\n",
      " 2 epoch  |  [ 2/2] batch\t|\tloss: 15.8553467\n",
      " 3 epoch  |  [ 1/2] batch\t|\tloss: 9.9122944\n",
      " 3 epoch  |  [ 2/2] batch\t|\tloss: 11.2258205\n",
      " 4 epoch  |  [ 1/2] batch\t|\tloss: 7.8855577\n",
      " 4 epoch  |  [ 2/2] batch\t|\tloss: 7.4867506\n",
      " 5 epoch  |  [ 1/2] batch\t|\tloss: 5.3244801\n",
      " 5 epoch  |  [ 2/2] batch\t|\tloss: 4.7884374\n",
      " 6 epoch  |  [ 1/2] batch\t|\tloss: 3.5725453\n",
      " 6 epoch  |  [ 2/2] batch\t|\tloss: 3.2939668\n",
      " 7 epoch  |  [ 1/2] batch\t|\tloss: 2.8895736\n",
      " 7 epoch  |  [ 2/2] batch\t|\tloss: 2.7233262\n",
      " 8 epoch  |  [ 1/2] batch\t|\tloss: 2.5642555\n",
      " 8 epoch  |  [ 2/2] batch\t|\tloss: 2.4937065\n",
      " 9 epoch  |  [ 1/2] batch\t|\tloss: 2.3682799\n",
      " 9 epoch  |  [ 2/2] batch\t|\tloss: 2.3376315\n",
      "10 epoch  |  [ 1/2] batch\t|\tloss: 2.2057655\n",
      "10 epoch  |  [ 2/2] batch\t|\tloss: 2.1621680\n",
      "11 epoch  |  [ 1/2] batch\t|\tloss: 2.0619085\n",
      "11 epoch  |  [ 2/2] batch\t|\tloss: 2.0742114\n",
      "12 epoch  |  [ 1/2] batch\t|\tloss: 1.9355989\n",
      "12 epoch  |  [ 2/2] batch\t|\tloss: 1.9709544\n",
      "13 epoch  |  [ 1/2] batch\t|\tloss: 1.7910074\n",
      "13 epoch  |  [ 2/2] batch\t|\tloss: 1.8475022\n",
      "14 epoch  |  [ 1/2] batch\t|\tloss: 1.6480200\n",
      "14 epoch  |  [ 2/2] batch\t|\tloss: 1.7297695\n",
      "15 epoch  |  [ 1/2] batch\t|\tloss: 1.5033793\n",
      "15 epoch  |  [ 2/2] batch\t|\tloss: 1.6191283\n",
      "16 epoch  |  [ 1/2] batch\t|\tloss: 1.3670254\n",
      "16 epoch  |  [ 2/2] batch\t|\tloss: 1.5002775\n",
      "17 epoch  |  [ 1/2] batch\t|\tloss: 1.2489264\n",
      "17 epoch  |  [ 2/2] batch\t|\tloss: 1.3755817\n",
      "18 epoch  |  [ 1/2] batch\t|\tloss: 1.1255785\n",
      "18 epoch  |  [ 2/2] batch\t|\tloss: 1.2635286\n",
      "19 epoch  |  [ 1/2] batch\t|\tloss: 1.0078930\n",
      "19 epoch  |  [ 2/2] batch\t|\tloss: 1.1492023\n",
      "20 epoch  |  [ 1/2] batch\t|\tloss: 0.8942596\n",
      "20 epoch  |  [ 2/2] batch\t|\tloss: 1.0439774\n",
      "21 epoch  |  [ 1/2] batch\t|\tloss: 0.8069967\n",
      "21 epoch  |  [ 2/2] batch\t|\tloss: 0.9389734\n",
      "22 epoch  |  [ 1/2] batch\t|\tloss: 0.7246625\n",
      "22 epoch  |  [ 2/2] batch\t|\tloss: 0.8618761\n",
      "23 epoch  |  [ 1/2] batch\t|\tloss: 0.6524458\n",
      "23 epoch  |  [ 2/2] batch\t|\tloss: 0.7716694\n",
      "24 epoch  |  [ 1/2] batch\t|\tloss: 0.5404067\n",
      "24 epoch  |  [ 2/2] batch\t|\tloss: 0.6518763\n",
      "25 epoch  |  [ 1/2] batch\t|\tloss: 0.4930274\n",
      "25 epoch  |  [ 2/2] batch\t|\tloss: 0.5656866\n",
      "26 epoch  |  [ 1/2] batch\t|\tloss: 0.4168885\n",
      "26 epoch  |  [ 2/2] batch\t|\tloss: 0.4827235\n",
      "27 epoch  |  [ 1/2] batch\t|\tloss: 0.3693596\n",
      "27 epoch  |  [ 2/2] batch\t|\tloss: 0.4097483\n",
      "28 epoch  |  [ 1/2] batch\t|\tloss: 0.3040356\n",
      "28 epoch  |  [ 2/2] batch\t|\tloss: 0.3711076\n",
      "29 epoch  |  [ 1/2] batch\t|\tloss: 0.2588900\n",
      "29 epoch  |  [ 2/2] batch\t|\tloss: 0.3010485\n",
      "30 epoch  |  [ 1/2] batch\t|\tloss: 0.2180665\n",
      "30 epoch  |  [ 2/2] batch\t|\tloss: 0.2575060\n",
      "31 epoch  |  [ 1/2] batch\t|\tloss: 0.1867252\n",
      "31 epoch  |  [ 2/2] batch\t|\tloss: 0.2083643\n",
      "32 epoch  |  [ 1/2] batch\t|\tloss: 0.1601126\n",
      "32 epoch  |  [ 2/2] batch\t|\tloss: 0.1764424\n",
      "33 epoch  |  [ 1/2] batch\t|\tloss: 0.1410746\n",
      "33 epoch  |  [ 2/2] batch\t|\tloss: 0.1415164\n",
      "34 epoch  |  [ 1/2] batch\t|\tloss: 0.1290683\n",
      "34 epoch  |  [ 2/2] batch\t|\tloss: 0.1183472\n",
      "35 epoch  |  [ 1/2] batch\t|\tloss: 0.1128879\n",
      "35 epoch  |  [ 2/2] batch\t|\tloss: 0.1079247\n",
      "36 epoch  |  [ 1/2] batch\t|\tloss: 0.1015818\n",
      "36 epoch  |  [ 2/2] batch\t|\tloss: 0.0926466\n",
      "37 epoch  |  [ 1/2] batch\t|\tloss: 0.0937500\n",
      "37 epoch  |  [ 2/2] batch\t|\tloss: 0.0806679\n",
      "38 epoch  |  [ 1/2] batch\t|\tloss: 0.0837911\n",
      "38 epoch  |  [ 2/2] batch\t|\tloss: 0.0710328\n",
      "39 epoch  |  [ 1/2] batch\t|\tloss: 0.0764069\n",
      "39 epoch  |  [ 2/2] batch\t|\tloss: 0.0635618\n",
      "40 epoch  |  [ 1/2] batch\t|\tloss: 0.0707950\n",
      "40 epoch  |  [ 2/2] batch\t|\tloss: 0.0581371\n",
      "41 epoch  |  [ 1/2] batch\t|\tloss: 0.0663438\n",
      "41 epoch  |  [ 2/2] batch\t|\tloss: 0.0507854\n",
      "42 epoch  |  [ 1/2] batch\t|\tloss: 0.0637217\n",
      "42 epoch  |  [ 2/2] batch\t|\tloss: 0.0507907\n",
      "43 epoch  |  [ 1/2] batch\t|\tloss: 0.0614003\n",
      "43 epoch  |  [ 2/2] batch\t|\tloss: 0.0551062\n",
      "44 epoch  |  [ 1/2] batch\t|\tloss: 0.0592047\n",
      "44 epoch  |  [ 2/2] batch\t|\tloss: 0.0535163\n",
      "45 epoch  |  [ 1/2] batch\t|\tloss: 0.0578666\n",
      "45 epoch  |  [ 2/2] batch\t|\tloss: 0.0495964\n",
      "46 epoch  |  [ 1/2] batch\t|\tloss: 0.0602061\n",
      "46 epoch  |  [ 2/2] batch\t|\tloss: 0.0432252\n",
      "47 epoch  |  [ 1/2] batch\t|\tloss: 0.0837916\n",
      "47 epoch  |  [ 2/2] batch\t|\tloss: 0.0611245\n",
      "48 epoch  |  [ 1/2] batch\t|\tloss: 0.0684600\n",
      "48 epoch  |  [ 2/2] batch\t|\tloss: 0.0563799\n",
      "49 epoch  |  [ 1/2] batch\t|\tloss: 0.0624071\n",
      "49 epoch  |  [ 2/2] batch\t|\tloss: 0.0531124\n",
      "50 epoch  |  [ 1/2] batch\t|\tloss: 0.0640934\n",
      "50 epoch  |  [ 2/2] batch\t|\tloss: 0.0508586\n",
      "51 epoch  |  [ 1/2] batch\t|\tloss: 0.0581907\n",
      "51 epoch  |  [ 2/2] batch\t|\tloss: 0.0378301\n",
      "52 epoch  |  [ 1/2] batch\t|\tloss: 0.0564810\n",
      "52 epoch  |  [ 2/2] batch\t|\tloss: 0.0441296\n",
      "53 epoch  |  [ 1/2] batch\t|\tloss: 0.0547775\n",
      "53 epoch  |  [ 2/2] batch\t|\tloss: 0.0408167\n",
      "54 epoch  |  [ 1/2] batch\t|\tloss: 0.0788885\n",
      "54 epoch  |  [ 2/2] batch\t|\tloss: 0.0296384\n",
      "55 epoch  |  [ 1/2] batch\t|\tloss: 0.0572333\n",
      "55 epoch  |  [ 2/2] batch\t|\tloss: 0.0889861\n",
      "56 epoch  |  [ 1/2] batch\t|\tloss: 0.0537708\n",
      "56 epoch  |  [ 2/2] batch\t|\tloss: 0.0364976\n",
      "57 epoch  |  [ 1/2] batch\t|\tloss: 0.0623653\n",
      "57 epoch  |  [ 2/2] batch\t|\tloss: 0.0316570\n",
      "58 epoch  |  [ 1/2] batch\t|\tloss: 0.0609481\n",
      "58 epoch  |  [ 2/2] batch\t|\tloss: 0.0356435\n",
      "59 epoch  |  [ 1/2] batch\t|\tloss: 0.0539560\n",
      "59 epoch  |  [ 2/2] batch\t|\tloss: 0.0319915\n",
      "60 epoch  |  [ 1/2] batch\t|\tloss: 0.0515183\n",
      "60 epoch  |  [ 2/2] batch\t|\tloss: 0.0198532\n",
      "61 epoch  |  [ 1/2] batch\t|\tloss: 0.0516115\n",
      "61 epoch  |  [ 2/2] batch\t|\tloss: 0.0179503\n",
      "62 epoch  |  [ 1/2] batch\t|\tloss: 0.0512960\n",
      "62 epoch  |  [ 2/2] batch\t|\tloss: 0.0174667\n",
      "63 epoch  |  [ 1/2] batch\t|\tloss: 0.0497352\n",
      "63 epoch  |  [ 2/2] batch\t|\tloss: 0.0140200\n",
      "64 epoch  |  [ 1/2] batch\t|\tloss: 0.0486081\n",
      "64 epoch  |  [ 2/2] batch\t|\tloss: 0.0168989\n",
      "65 epoch  |  [ 1/2] batch\t|\tloss: 0.0481056\n",
      "65 epoch  |  [ 2/2] batch\t|\tloss: 0.0132487\n",
      "66 epoch  |  [ 1/2] batch\t|\tloss: 0.0480416\n",
      "66 epoch  |  [ 2/2] batch\t|\tloss: 0.0123559\n",
      "67 epoch  |  [ 1/2] batch\t|\tloss: 0.0478877\n",
      "67 epoch  |  [ 2/2] batch\t|\tloss: 0.0087400\n",
      "68 epoch  |  [ 1/2] batch\t|\tloss: 0.0471959\n",
      "68 epoch  |  [ 2/2] batch\t|\tloss: 0.0080324\n",
      "69 epoch  |  [ 1/2] batch\t|\tloss: 0.0468451\n",
      "69 epoch  |  [ 2/2] batch\t|\tloss: 0.0075375\n",
      "70 epoch  |  [ 1/2] batch\t|\tloss: 0.0465579\n",
      "70 epoch  |  [ 2/2] batch\t|\tloss: 0.0065409\n",
      "71 epoch  |  [ 1/2] batch\t|\tloss: 0.0464355\n",
      "71 epoch  |  [ 2/2] batch\t|\tloss: 0.0057849\n",
      "72 epoch  |  [ 1/2] batch\t|\tloss: 0.0463277\n",
      "72 epoch  |  [ 2/2] batch\t|\tloss: 0.0050983\n",
      "73 epoch  |  [ 1/2] batch\t|\tloss: 0.0460534\n",
      "73 epoch  |  [ 2/2] batch\t|\tloss: 0.0046360\n",
      "74 epoch  |  [ 1/2] batch\t|\tloss: 0.0458407\n",
      "74 epoch  |  [ 2/2] batch\t|\tloss: 0.0042653\n",
      "75 epoch  |  [ 1/2] batch\t|\tloss: 0.0455865\n",
      "75 epoch  |  [ 2/2] batch\t|\tloss: 0.0039030\n",
      "76 epoch  |  [ 1/2] batch\t|\tloss: 0.0454268\n",
      "76 epoch  |  [ 2/2] batch\t|\tloss: 0.0036707\n",
      "77 epoch  |  [ 1/2] batch\t|\tloss: 0.0452743\n",
      "77 epoch  |  [ 2/2] batch\t|\tloss: 0.0034993\n",
      "78 epoch  |  [ 1/2] batch\t|\tloss: 0.0450965\n",
      "78 epoch  |  [ 2/2] batch\t|\tloss: 0.0033416\n",
      "79 epoch  |  [ 1/2] batch\t|\tloss: 0.0449271\n",
      "79 epoch  |  [ 2/2] batch\t|\tloss: 0.0031983\n",
      "80 epoch  |  [ 1/2] batch\t|\tloss: 0.0448313\n",
      "80 epoch  |  [ 2/2] batch\t|\tloss: 0.0030413\n",
      "81 epoch  |  [ 1/2] batch\t|\tloss: 0.0447307\n",
      "81 epoch  |  [ 2/2] batch\t|\tloss: 0.0028655\n",
      "82 epoch  |  [ 1/2] batch\t|\tloss: 0.0446585\n",
      "82 epoch  |  [ 2/2] batch\t|\tloss: 0.0027033\n",
      "83 epoch  |  [ 1/2] batch\t|\tloss: 0.0445831\n",
      "83 epoch  |  [ 2/2] batch\t|\tloss: 0.0025699\n",
      "84 epoch  |  [ 1/2] batch\t|\tloss: 0.0445232\n",
      "84 epoch  |  [ 2/2] batch\t|\tloss: 0.0024554\n",
      "85 epoch  |  [ 1/2] batch\t|\tloss: 0.0444506\n",
      "85 epoch  |  [ 2/2] batch\t|\tloss: 0.0023540\n",
      "86 epoch  |  [ 1/2] batch\t|\tloss: 0.0443656\n",
      "86 epoch  |  [ 2/2] batch\t|\tloss: 0.0022684\n",
      "87 epoch  |  [ 1/2] batch\t|\tloss: 0.0442791\n",
      "87 epoch  |  [ 2/2] batch\t|\tloss: 0.0021978\n",
      "88 epoch  |  [ 1/2] batch\t|\tloss: 0.0442085\n",
      "88 epoch  |  [ 2/2] batch\t|\tloss: 0.0021345\n",
      "89 epoch  |  [ 1/2] batch\t|\tloss: 0.0441458\n",
      "89 epoch  |  [ 2/2] batch\t|\tloss: 0.0020715\n",
      "90 epoch  |  [ 1/2] batch\t|\tloss: 0.0440892\n",
      "90 epoch  |  [ 2/2] batch\t|\tloss: 0.0020074\n",
      "91 epoch  |  [ 1/2] batch\t|\tloss: 0.0440397\n",
      "91 epoch  |  [ 2/2] batch\t|\tloss: 0.0019431\n",
      "92 epoch  |  [ 1/2] batch\t|\tloss: 0.0439970\n",
      "92 epoch  |  [ 2/2] batch\t|\tloss: 0.0018801\n",
      "93 epoch  |  [ 1/2] batch\t|\tloss: 0.0439587\n",
      "93 epoch  |  [ 2/2] batch\t|\tloss: 0.0018209\n",
      "94 epoch  |  [ 1/2] batch\t|\tloss: 0.0439208\n",
      "94 epoch  |  [ 2/2] batch\t|\tloss: 0.0017669\n",
      "95 epoch  |  [ 1/2] batch\t|\tloss: 0.0438837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 epoch  |  [ 2/2] batch\t|\tloss: 0.0017180\n",
      "96 epoch  |  [ 1/2] batch\t|\tloss: 0.0438479\n",
      "96 epoch  |  [ 2/2] batch\t|\tloss: 0.0016733\n",
      "97 epoch  |  [ 1/2] batch\t|\tloss: 0.0438122\n",
      "97 epoch  |  [ 2/2] batch\t|\tloss: 0.0016325\n",
      "98 epoch  |  [ 1/2] batch\t|\tloss: 0.0437757\n",
      "98 epoch  |  [ 2/2] batch\t|\tloss: 0.0015950\n",
      "99 epoch  |  [ 1/2] batch\t|\tloss: 0.0437411\n",
      "99 epoch  |  [ 2/2] batch\t|\tloss: 0.0015599\n",
      "100 epoch  |  [ 1/2] batch\t|\tloss: 0.0437085\n",
      "100 epoch  |  [ 2/2] batch\t|\tloss: 0.0015264\n",
      "학습 끝\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "learning_rate = 0.02\n",
    "num_epochs = 100\n",
    "num_hidden_layers = 2\n",
    "grad_clip = 5\n",
    "\n",
    "## graph\n",
    "\n",
    "# reset graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 인풋/타겟 데이터, 배치 사이즈를 입력받기 위한 플레이스홀더\n",
    "input_data = tf.placeholder(tf.int32, shape=[None, None])  # input: [batch_size, seq_length]\n",
    "target_data = tf.placeholder(tf.int32, shape=[None, None]) # target: [batch_size, seq_length]\n",
    "state_batch_size = tf.placeholder(tf.int32, shape=[])\n",
    "\n",
    "# RNN 마지막 히든레이어 출력을 소프트맥스 출력값으로 변환해주기 위한 변수\n",
    "softmax_w = tf.Variable(tf.random_normal(shape=[hidden_size, vocab_size]), dtype=tf.float32)\n",
    "softmax_b = tf.Variable(tf.random_normal(shape=[vocab_size]), dtype=tf.float32)\n",
    "\n",
    "# 히든레이어 수 만큼 LSTM cell(히든레이어) 선언\n",
    "cells = []\n",
    "for _ in range(num_hidden_layers):\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "    cells.append(cell)\n",
    "    \n",
    "# cell을 종합해서 RNN을 정의\n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "# 인풋데이터를 변환하기 위한 임베딩 매트릭스 선언\n",
    "# vocab_size -> hidden_size\n",
    "embedding = tf.Variable(tf.random_normal(shape=[vocab_size, hidden_size]), dtype=tf.float32)\n",
    "inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "\n",
    "# 초기 state 값을 0으로 초기화\n",
    "initial_state = cell.zero_state(state_batch_size, tf.float32)\n",
    "\n",
    "# 학습을 위한 tf.nn.dynamic_rnn을 선언\n",
    "# outputs: [batch_size, seq_length, hidden_size]\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# output을 [batch_size * seq_length, hidden_size] shape으로 바꿈\n",
    "output = tf.reshape(outputs, [-1, hidden_size])\n",
    "\n",
    "# 최종 출력값을 설정\n",
    "# logits: [batch_size * seq_length, vocab_size]\n",
    "# softmax 를 적용하기 위해 vocab_size 로 shape 을 바꾼다.\n",
    "# output.shape: (?,64)\n",
    "# logits.shape: (?,36)\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "probs = tf.nn.softmax(logits)\n",
    "\n",
    "# 크로스 엔트로피 손실함수 정의\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=target_data))\n",
    "\n",
    "# 옵티마이저 선언하고 옵티마이저에 Gradient Clipping을 적용\n",
    "# grad_clip 보다 큰 Gradient를 5로 Clipping 한다\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "## 세션을 열고 학습 진행\n",
    "# with tf.Session() as sess:\n",
    "sess = tf.Session()\n",
    "# 변수에 초기값 할당\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    data_loader.reset_batch_pointer() # TextLoader 의 배치포인터 리셋\n",
    "    # 초기 상태값 지정\n",
    "    state = sess.run(initial_state, feed_dict={state_batch_size: batch_size})\n",
    "\n",
    "    for b in range(data_loader.total_batch):\n",
    "        # x,y 데이터 불러오기\n",
    "        x, y = data_loader.next_batch()\n",
    "        # y에 one-hot 인코딩 적용\n",
    "        y = tf.one_hot(y, vocab_size)        # y: [batch_size, seq_length, vocab_size]\n",
    "        y = tf.reshape(y, [-1, vocab_size])  # y: [batch_size * seq_length, vocab_size]\n",
    "        y = y.eval(session=sess)\n",
    "\n",
    "        # feed_dict 에 사용할 값과 LSTM 초기 cell state(feed_dict[c])값과 hidden layer 출력값(feed_dict[h])를 지정\n",
    "        feed_dict = {input_data: x, target_data: y, state_batch_size: batch_size}\n",
    "        for i, (c, h) in enumerate(initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "\n",
    "        # 1스텝 학습을 진행\n",
    "        _, loss_print, state = sess.run([train_step, loss, final_state], feed_dict=feed_dict)\n",
    "\n",
    "        print(f'{e+1:3d} epoch  |  [{b+1:2d}/{data_loader.total_batch}] batch\\t|\\tloss: {loss_print:.7f}')\n",
    "\n",
    "print('학습 끝')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플링 시작\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# 샘플링 시작\n",
    "print('샘플링 시작')\n",
    "num_sampling = 4000   # 생성할 글자 수 지정\n",
    "prime = u' '          # 시작 글자를 ' '(공백)으로 지정\n",
    "sampling_type = 1     # 샘플링 타입 설정\n",
    "state = sess.run(cell.zero_state(1, tf.float32)) # RNN 최초 state값을 0으로 초기화\n",
    "\n",
    "# 랜덤 샘플링을 위한 weight_pick 함수 정의\n",
    "def weighted_pick(weights):\n",
    "    t = np.cumsum(weights)\n",
    "    s = np.sum(weights)\n",
    "    return int(np.searchsorted(t, np.random.rand(1)*s))\n",
    "\n",
    "ret = prime\n",
    "char = prime[-1]\n",
    "\n",
    "for n in range(num_sampling):\n",
    "    x = np.zeros((1,1))\n",
    "    x[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object of too small depth for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-c64263dd79f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msearchsorted\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msearchsorted\u001b[1;34m(a, v, side, sorter)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m     \"\"\"\n\u001b[1;32m-> 1296\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'searchsorted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mside\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msorter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: object of too small depth for desired array"
     ]
    }
   ],
   "source": [
    "# 랜덤 샘플링을 위한 weight_pick 함수 정의\n",
    "def weighted_pick(weights):\n",
    "    t = np.cumsum(weights)\n",
    "    s = np.sum(weights)\n",
    "    return int(np.searchsorted(t, np.random.rand(1)*s))\n",
    "\n",
    "np.searchsorted(2, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
